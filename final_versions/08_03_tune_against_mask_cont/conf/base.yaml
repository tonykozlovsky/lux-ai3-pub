defaults:
  - _self_
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


hydra:
  run:
    dir: ./outputs/${now:%m-%d}/${now:%H-%M-%S}


wandb:
  project: "2025"
  entity: ${oc.env:WANDB_USER, "user"}
  group: ${oc.env:WANDB_GROUP, "lux_s3_experiments"}


## ENV params
act_space: BasicActionSpace
obs_space: Obs3

#GameResultReward or StatefulMultiReward or GameWinReward
reward_space: GameResultReward
enable_game_early_stop: False # if True GameResultReward max min should be +3 -3 else +7.5 -7.5 BUT NOW True DEPRECATED, SHOULD BE False with -7.5 +7.5 and GameResultReward

# False if using GameResultReward or GameWinReward
enable_multiplier_scaling: False

five_rounds: True
flip_learning: True
flip_axes: True
enable_sap_masks: False
use_GPT_losses: False

zero_energy_on_invalidation: False

use_old_input: False


disable_wandb: False
debug: False # unused ?


## TRAINING params

total_steps: 2e8
unroll_length: 16
discounting: 0.999
clip_grads: 0



## OPTIMIZER params
optimizer_class: Adam
optimizer_kwargs:
  lr: 1e-5
  # See https://arxiv.org/pdf/2105.05246.pdf
  eps: 0.0003
  #alpha: 0.9
min_lr_mod: 0.01
enable_lr_scheduler: True
lr_lambda: 1
lr_lambda_change_per_step: -0.5e-8

baseline_cost: 1.
# lambda parameter for TD-lambda and UPGO losses
lmb: 0.9

reduction: mean

stats_freq_batches: 1
# file_descriptor or file_system
sharing_strategy: file_descriptor

# for run one test game without learning
replay: False
kaggle: False


# user@host:/lux_ai/toad_fork$ pip3 install ../Lux-Design-S3/src/ && TRAINING=1 USE_TORCH_COMPILE=1 TORCHINDUCTOR_COMPILE_THREADS=1 SMALL=0 BENCHMARK=1 python3 ./run_monobeast.py

# pip3 install ../Lux-Design-S3/src/ && TRAINING=1 USE_TORCH_COMPILE=0 SMALL=0 BENCHMARK=1 CUDA_VISIBLE_DEVICES=1 python3 ./run_monobeast.py

# pip3 install ../Lux-Design-S3/src/ && TRAINING=1 USE_TORCH_COMPILE=1 SMALL=0 BENCHMARK=1 CUDA_VISIBLE_DEVICES=3 python3 ./run_monobeast.py


benchmark_baseline_dir: ./models/01_03_test_x8_cont_7/
benchmark_baseline_file: 200000000_weights.pt

competitors_dir: ./outputs/03-06/16-17-09/

num_competitors:  40
num_bench_envs:  10  # replaces n_actor_envs param

fixed_baseline: True
rerun: True
start_from_checkpoint: 240168320_weights.pt # starts from first checkpoint if not specified
games_to_test: 3200


