# ulimit -c 0 && pip3 install ../Lux-Design-S3/src/ && WANDB_API_KEY=_ WANDB_MODE=offline TRAINING=1 USE_TORCH_COMPILE=1 SMALL=0 python3 ./run_monobeast.py --config-name=two_gpu

defaults:
  - base  # Inherits from base.yaml

enable_resnet: True
enable_lstm: True # High memory consumption when enabled
lstm_k_size: 3

enable_transformer: False # DEPRECATED
n_transformer_blocks: 1 # DEPRECATED

use_baseline_v2: True # False DEPRECATED should be True for new models

enable_transformer_v2: True
n_transformer_v2_blocks: 4

transformer_dim_head: 32 # 32 HERE to match 128 hidden dim

one_prediction_head: True # False DEPRECATED should be True for new models
enable_per_unit_resnet: True # HEAVY
enable_se_in_per_unit_resnet: False # True DEPRECATED should be False for new models

use_embedding_input: False # True DEPRECATED should be False for new models

#_________________________________

name: tune_against_mask

#_________________________________

use_bf16: True

num_actors: 100
num_buffers: 150
# bus error if not enough /dev/shm
# 53 buffers of n_actor_envs=8 consumes 34G of /dev/shm (~82mb per env) WITH LSTM, WITHOUT - MUCH SMALLER !!!

num_stats_buffers: 16

n_actor_envs: 20
batch_size: 20

n_batch_prepare_processes: 2
prepare_batches: 3

## MODEL params
model_arch: conv_model
n_blocks: 24
hidden_dim: 128
embedding_dim: 32
n_merge_layers: 1
normalize: False # False better but trains slower
# Conv-specific params
kernel_size: 5


## LOSS params

prediction_cost: 1. # ???????? May be too much slower learning with 1. ?

target_entropy_worker: 0.2
target_entropy_worker_change_per_step: -0.066666e-8
target_entropy_sapper: 0.6
target_entropy_sapper_change_per_step: -0.2e-8
entropy_initial_multiplier_worker: 0.001
entropy_initial_multiplier_sapper: 0.01
entropy_mult_change_speed_per_step: 0.00000080000


# Pretrained model for KL loss
use_teacher: True
use_old_teacher_loss: False # Not working if LSTM enabled in teacher
only_teacher_loss: False
frozen_teacher_both_sides: False
frozen_teacher_sample: False

teacher_load_dir: ./models/01_03_test_x8_cont_7/
teacher_checkpoint_file: 200000000_weights.pt

teacher_kl_cost: 0.02
teacher_kl_cost_change_per_step: 0.
teacher_baseline_cost: 0.01

imitation_cost: 0.

# MISCELLANEOUS params
n_learner_devices: 4
n_actor_devices: 4

update_actor_weigts_every_batches: 1 # is it needed ?


# Continue from previous run
load_dir: ./models/01_03_test_x8_cont_7/
checkpoint_file: 200000000_weights.pt
weights_only: True
load_optimizer: False
n_warmup_steps: 1000000


# Enabled if  num_frozen_model_actors > 0
frozen_actor_checkpoint_paths:
  - ./models/06_03_mask_exp/200000000_weights.pt
change_frozen_actor_every_n_iters: 1000

num_frozen_model_actors: 33    # its 33% of num_actors
num_frozen_model_actors_buffers: 50 # bus error if not enough /dev/shm


frozen_teacher_actors: 33  # NOT TESTED PROPERLY
frozen_teacher_models_buffers: 50  # bus error if not enough /dev/shm


behavior_cloning_actors: 0 # NOT WORKING YET
behavior_cloning_actors_buffers: 0 # bus error if not enough /dev/shm


# guarantee batches of different types ratio not depending on num_actors
frozen_actor_probability: 0.33
frozen_teacher_probability: 0.33
behavior_cloning_probability: 0. # NOT WORKING YET
selfplay_probability: 1. # all remaining probability is for selfplay